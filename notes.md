- Stochastic optimization = black-box optimization, pq a gnt **n conheçe a função objetivo**, daí ela é uma caixa preta, vc pode perguntar qual é o valor pra uma solução mas vc n pode conhecer a função pra poder **achar seu gradiente** e fazer descida de gradiente.
- São úteis pro TSP: tem algoritmos exatos (uma família de algo q funciona bem pra TSP são os de programação dinâmica) q vão achar o resultado ótimo seguindo uma heurística (enq os genéticos são chamados de meta-heurísticos) e q convergem bem rápido em pequenas dimensões, mas pro TSP de altas dimensões (mtas cidades) os algoritmos genéticos são melhores q as outras soluções
  - Eles n garantem necessariamente o melhor resultado, mas qnd tem mtas cidades eles n demoram tanto
  - Detalhe tbm q eles são utilizáveis msm se a função custo n é uma distância euclidiana óbvia
- Uma coisa boa é q, diferentemente do caso com otimização contínua, os algo evolucionários n seguem um único ponto dps do outro, ele pesquisa em volta de cada ponto pra ter uma ideia da função objetivo ent é mais robusto contra ótimos locais
- Outra coisa dahora é a **criatividade desses algoritmos**, por ex, tem um paper no slide que procurava criar uma moeda comemorativa com o tema de tecnologia, IA, etc. Mas, pedindo pra uma IA generativa gerar um eles viram q n era nada criativo, **só clichês**: uma cabeça pra esquerda, uma mistura de cérebro e circuito, etc. Ent fizeram com algoritmos genéticos q eram avaliados por artistas e saiu algo realmente inesperado